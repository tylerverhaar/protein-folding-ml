import time, pickle
from spektral.data import Dataset, BatchLoader, PackedBatchLoader, Loader
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from datetime import datetime
import os, sys
import numpy as np
#from decoys import Decoys
from modelling.decoys import Decoys
from tqdm import tqdm
import json
import tensorflow as tf
import matplotlib.pyplot as plt
from typing import Any, Dict, Optional, List, Union
#from tools.mathlib import compute_expected_value, adjacency_matrix_map
import pandas as pd
import keras
import tensorflow as tf

from modelling.tools import convert_to_one_hot, get_expected_rmsd
from modelling.classifier_analysis import ClassifierAnalysis
from modelling.ranking_analysis import RankingAnalysis

import os
#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'



class TrainTestModule:
    """
    Class for training and testing a model using the Decoys dataset.

    Args:
        model_dir (str, optional): Directory to store models and results. Defaults to 'modelling/'.
    """

    def __init__(self, model_dir: Optional[str] = 'modelling/') -> None:
        """
        Initialize the TrainTestModule class.

        Args:
            model_dir (str, optional): Directory to store models and results. Defaults to 'modelling/'.
        """
        self.model_dir = model_dir
        self.models_dir = f'{model_dir}/models'
        self.results_dir = f'{model_dir}/results'
        self.dataset = Decoys()
        dataset_MB = sys.getsizeof(self.dataset) / (1024**2)
        self.n_decoys = len(self.dataset)
        print(f'self.dataset size (MB): {dataset_MB}')
        print(f'self.dataset length: {self.n_decoys}')

    def _get_next_filename(self, directory: str, prefix: str, suffix: Optional[str] = None, **kwargs: Dict[str, Any]) -> str:
        """
        Get the next available filename for writing in the specified directory.

        This function reads the filenames in the directory, extracts the maximum weight number from
        the existing filenames that start with the given prefix, and determines the next filename to write.
        The next filename is generated by incrementing the maximum weight number by 1.

        Args:
            directory (str): The directory path where the filenames are located.
            prefix (str): The prefix of the filenames.
            suffix (Optional[str]): The suffix or file extension of the filenames. Optional, set to None.
            kwargs (Dict[str, Any]): Additional keyword arguments.

        Returns:
            str: The next available filename for writing.

        Raises:
            FileNotFoundError: If the specified directory does not exist.
        """
        filenames = os.listdir(directory)
        matching_filenames = [filename for filename in filenames if filename.startswith(prefix)]
        if len(matching_filenames) > 0:
            weights = [int(filename.split('_')[-1].split('.')[0]) for filename in matching_filenames]
            max_weight = max(weights) if weights else 0
            next_weight = max_weight + 1
        else:
            next_weight = 1
        next_filename = f"{directory}/{prefix}_{next_weight}.{suffix}" if suffix is not None else f"{directory}/{prefix}_{next_weight}"
        return next_filename

    def _get_most_recent_file(self, directory: str, prefix: str, suffix: str, **kwargs: Dict[str, Any]) -> str:
        """
        Get the most recent file in the specified directory with the given prefix and suffix.

        This function reads the filenames in the directory, filters them based on the provided prefix
        and suffix, and returns the filename with the most recent modification timestamp.

        Args:
            directory (str): The directory path where the filenames are located.
            prefix (str): The prefix of the filenames.
            suffix (str): The suffix or file extension of the filenames.

        Returns:
            str: The most recent file name with the specified prefix and suffix.

        Raises:
            FileNotFoundError: If the specified directory does not exist.
            ValueError: If no files matching the prefix and suffix are found.
        """
        filenames = [filename for filename in os.listdir(directory) if filename.startswith(prefix) and filename.endswith(suffix)]

        if not filenames:
            raise ValueError("No files matching the prefix and suffix found.")

        most_recent_file = max(filenames, key=lambda x: os.path.getmtime(os.path.join(directory, x)))
        return f'{directory}/{most_recent_file}'

    def _set_training_params(self, params: Dict[str, Any], **kwargs: Dict[str, Any]):
        self.params = params
        self.train_size = params['train_size']
        self.batch_size_tr = params['batch_size_tr']
        self.epochs_tr = params['epochs_tr']

    def _set_train_test_split_index(self, train_size: float, **kwargs: Dict[str, Any]) -> None:
        """
        Set the split index for train-test split.

        Args:
            train_size (float): Proportion of data to use for training.

        Raises:
            AssertionError: If train_size is not in the range [0, 1].
        """
        if hasattr(self, 'split_index'):
            return
        assert 0 <= train_size and train_size <= 1, 'ValueError, train_size must be in [0,1]'
        self.train_size = train_size
        n = len(self.dataset)
        split_index = int(n * self.train_size)
        split_index = split_index - (split_index % 500)
        self.split_index = split_index

        decoy_data = pd.read_csv(f'{self.results_dir}/decoys.csv')
        import pdb; pdb.set_trace()
        while True:
            protein_id0 = decoy_data.iloc[self.split_index-1]['protein_id']
            protein_id1 = decoy_data.iloc[self.split_index]['protein_id']
            if protein_id0 != protein_id1:
                break
            else:
                self.split_index += 500
            
        decoy_data = None

    def _set_train_loader(self, train_size: float, batch_size: int, epochs: int, **kwargs: Dict[str, Any]) -> None:
        """
        Set the data loader for the training set.

        Args:
            train_size (float): Proportion of data to use for training.
            batch_size (int): Batch size for training.
            epochs (int): Number of training epochs.
        """
        self.training_params = {'train_size' : train_size, 'batch_size' : batch_size, 'epochs' : epochs}
        self._set_train_test_split_index(train_size = train_size)
        self.loader_tr = BatchLoader(
            self.dataset[0:self.split_index], 
            batch_size = batch_size, 
            epochs = epochs, 
            node_level = False
            )

    def _reset_train_loader(self, **kwargs: Dict[str, Any]) -> None:
        """
        Reset the train loader with updated parameters.

        This function resets the train loader using the specified parameters or the default values.
        It updates the train size, batch size, and optionally the number of epochs for the train loader.

        Args:
            kwargs (Dict[str, Any]): Additional keyword arguments.
                - 'epochs' (int, optional): Number of epochs for the train loader. If not provided,
                the default value set during initialization will be used.

        Returns:
            None.
        """
        train_size = kwargs.get('train_size', self.training_params['train_size'])
        batch_size = kwargs.get('batch_size', self.training_params['batch_size'])
        epochs = kwargs.get('epochs', self.training_params['epochs'])
        self._set_train_loader(train_size = train_size, batch_size = batch_size, epochs = epochs)

    def _reset_test_loader(self, **kwargs: Dict[str, Any]) -> None:
        train_size = kwargs.get('train_size', self.testing_params['train_size'])
        batch_size = kwargs.get('batch_size', self.testing_params['batch_size'])
        epochs = kwargs.get('epochs', self.testing_params['epochs'])
        self._set_test_loader(train_size = train_size, batch_size = batch_size, epochs = epochs)

    def _set_test_loader(self, train_size: float, batch_size: Optional[int] = 32, epochs: Optional[int] = None, **kwargs: Dict[str, Any]) -> None:
        """
        Set the data loader for the test set.

        Args:
            train_size (float): Proportion of data to use for training.
            batch_size (int, optional): Batch size for testing. Defaults to 500.
        """
        self.testing_params = {'train_size' : train_size, 'batch_size' : batch_size, 'epochs' : epochs}
        self._set_train_test_split_index(train_size = train_size)
        self.loader_te = BatchLoader(
            self.dataset[self.split_index:], 
            batch_size = batch_size, 
            epochs = epochs,
            shuffle = False, 
            node_level = False
            )

    def _set_train_test_loaders(self, 
                                train_size: float, 
                                batch_size_tr: Optional[int] = 32, 
                                batch_size_te: Optional[int] = 32,
                                epochs_tr: Optional[int] = None, 
                                epochs_te: Optional[int] = None, 
                                **kwargs: Dict[str, Any]) -> None:
        """
        Set the data loaders for both training and testing.

        Args:
            train_size (float): Proportion of data to use for training.
            batch_size_tr (int): Batch size for training.
            epochs_tr (int): Number of training epochs.
            batch_size_te (int, optional): Batch size for testing. Defaults to 500.
        """
        self._set_train_test_split_index(train_size = train_size)
        self._set_train_loader(train_size = train_size, batch_size = batch_size_tr, epochs = epochs_tr)
        train_loader_MB = sys.getsizeof(self.loader_tr) / (1024**2)
        print(f'train_loader (GB): {train_loader_MB / 1000:.4}')
        self._set_test_loader(train_size = train_size, batch_size = batch_size_te, epochs = epochs_te)
        test_loader_MB = sys.getsizeof(self.loader_te) / (1024**2)
        print(f'test_loader (GB): {test_loader_MB / 1000:.4}')

    def _set_gpu_config(self):
        """
        Set GPU configuration for TensorFlow.

        This function detects the presence of a GPU and enables memory growth to use the GPU.
        """
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
            try:
                tf.config.set_logical_device_configuration(
                    gpus[0],
                    [tf.config.LogicalDeviceConfiguration(memory_limit = 2 * 4096)])
                logical_gpus = tf.config.list_logical_devices('GPU')
                print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
            except RuntimeError as e:
                # Virtual devices must be set before GPUs have been initialized
                print(e)

    def _save_model(self, **kwargs: Dict[str, Any]) -> None:
        """
        Save a TensorFlow Keras model.

        Args:
            **kwargs: Additional arguments to be passed to the model's save method.

        Returns:
            None
        """
        # save model
        #model_save_filepath = os.path.join(self.model_directory, "model.h5")
        #self.model.save(model_save_filepath, save_format = 'tf')

        # save weights
        model_weights_filepath = self._get_next_filename(directory = self.model_directory, prefix = 'weights', suffix = 'h5')
        print(model_weights_filepath, self.model_directory)
        #model_weights_filepath = os.path.join(self.model_directory, "weights.h5")
        self.model.save_weights(model_weights_filepath)

        with open(f'{self.model_directory}/summary.txt', 'w') as f:
            self.model.summary(print_fn=lambda x: f.write(x + '\n'))

        # save structure
        #model_plot_filepath = self._get_next_filename(directory = self.model_directory, prefix = 'plot', suffix = 'png')
        #tf.keras.utils.plot_model(model = self.model, to_file = model_plot_filepath, show_shapes = True)

    def _set_model_directory(self, **kwargs: Dict[str, Any]) -> None:
        """
        Set the model and result directories based on the current timestamp.

        Args:
            kwargs: Additional keyword arguments.

        Returns:
            None
        """
        model_name = kwargs.get('model_name', None)
        if model_name:
            self.model_directory = os.path.join(self.models_dir, model_name)
            self.result_directory = os.path.join(self.results_dir, model_name)
        else:
            datetime_now = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

            self.model_directory = os.path.join(self.models_dir, datetime_now)
            os.makedirs(self.model_directory, exist_ok = False)

            self.result_directory = os.path.join(self.results_dir, datetime_now)
            os.makedirs(self.result_directory, exist_ok = False)
        
    def set_model(self, model: tf.keras.models.Model, **kwargs: Dict[str, Any]) -> None:
        """
        Set the model either by loading a pre-existing model or setting a new model.

        Args:
            model (Union[str, tf.keras.models.Model]): The model to be set. It can be either a string representing the
                path to a pre-existing model directory or a tf.keras.models.Model object.
            **kwargs (Dict[str, Any]): Additional keyword arguments.

        Raises:
            Exception: If the model cannot be set.

        Returns:
            None
        """
        self._set_model_directory(**kwargs)
        learning_rate = kwargs.get('learning_rate', 0.001)
        metrics = kwargs.get('metrics', ['accuracy'])
        model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = learning_rate), loss = 'categorical_crossentropy', metrics = metrics)
        self.model = model
        model_name = kwargs.get('model_name', None)
        if model_name:
            import pdb; pdb.set_trace()
            weights_path = self._get_most_recent_file(directory = self.model_directory, prefix = 'weights', suffix = 'h5')
            self.model.built = True
            self.model.load_weights(weights_path)
      
    def _set_model_checkpointing(self, **kwargs: Dict[str, Any]) -> None:
        """
        Set up model checkpointing during training.

        Args:
            kwargs: Additional keyword arguments.
                - monitor: List of metrics to monitor for checkpointing (default: ['loss', 'accuracy', 'val_loss', 'val_accuracy', 'precision', 'recall']).
        """
        if hasattr(self, 'callbacks_list'):
            return
        
        monitor = kwargs.get('monitor', ['loss', 'accuracy', 'val_loss', 'val_accuracy', 'precision', 'recall'])
        checkpoint_path = os.path.join(self.model_directory, 'checkpoints')
        checkpoint_path = self._get_next_filename(directory = self.model_directory, prefix = 'checkpoints')
        checkpoint = tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path,
            monitor = monitor,
            verbose = 1,
            save_best_only = False,
            mode = 'min',
            save_weights_only = True
        )
        self.callbacks_list = [checkpoint]
        
    def _save_callback_history(self, callback_hist: tf.keras.callbacks.History, **kwargs: Dict[str, Any]) -> None:
        """
        Save the callback history to a JSON file.

        Args:
            callback_hist (tf.keras.callbacks.History): Callback history object containing training metrics.
            **kwargs (Dict[str, Any]): Additional arguments.

        Returns:
            None
        """
        callback_filepath = self._get_next_filename(directory = self.model_directory, prefix = 'training_metrics', suffix = 'json')
        #callback_filepath = os.path.join(self.model_directory, 'training_metrics_w1.json')
        
        with open(callback_filepath, "w") as file:
            json.dump(callback_hist.history, file)
        
    def _train(self, epochs: int, **kwargs: Dict[str, Any]) -> None:
        """
        Train the model using the provided loader and training parameters.

        Args:
            **kwargs (Dict[str, Any]): Additional keyword arguments for training.

        Raises:
            AttributeError: If the `model` attribute is not defined.
            AttributeError: If the `model_directory` attribute is not defined.

        Returns:
            None
        """
        if not hasattr(self, 'model'):
            raise AttributeError("AttributeError: TrainTestModule has no attribute: model")
        if not hasattr(self, 'model_directory'):
            raise AttributeError("AttributeError: TrainTestModule has no attribute: model_directory")
        
        loader = self.loader_tr
        steps_per_epoch = kwargs.get('steps_per_epoch', loader.steps_per_epoch)
        self._set_model_checkpointing()
        callbacks_list = self.callbacks_list
        callback_hist = self.model.fit(
            loader.load(), 
            steps_per_epoch = steps_per_epoch, 
            epochs = epochs, 
            callbacks = callbacks_list,
            validation_data = self.loader_te,
            validation_steps = self.loader_te.steps_per_epoch,
        )
        
        self._save_model(**kwargs)
        self._save_callback_history(callback_hist=callback_hist, **kwargs)
        
    def _evaluate_results(self, train_test: str, **kwargs: Dict[str, Any]) -> Dict[str, Any]:

        if train_test == 'train':
            if not hasattr(self, 'train_results'):
                raise ArithmeticError('TrainTestModule has no attribute train_results, required for _evaluate with train_test = "train"')
            else:
                results = self.train_results
        
        if train_test == 'test':
            if not hasattr(self, 'test_results'):
                raise ArithmeticError('TrainTestModule has no attribute test_results, required for _evaluate with train_test = "test"')
            else:
                results = self.test_results
        
        # choose results dictionary and iloc index of decoy data
        iloc_idx = np.arange(self.split_index, self.n_decoys) if train_test == 'test' else np.arange(0, self.split_index)

        # read decoy data
        decoys = pd.read_csv(f'{self.results_dir}/decoys.csv').iloc[iloc_idx]
        decoys.reset_index(inplace = True)
        assert decoys.shape[0] == results['n'], f'AssertionError, requires n = {decoys.shape[0]}'


        y_pred_onehot = convert_to_one_hot(arr = results['y_prob'])
        clf_analysis = ClassifierAnalysis(y_true = results['y_true'], y_pred = y_pred_onehot, y_prob = results['y_prob'])
        clf_results = clf_analysis.get_results()

        predictions = pd.DataFrame({key: results[key] for key in ['protein_id', 'decoy_id', 'y_pred_cts']}).rename(columns = {'protein_id' : 'protein_number'})
        data = pd.concat([predictions, decoys], axis=1).rename(columns = {'y_pred_cts' : 'expected_rmsd'})

        # Compute the crystal ball groupby dictionary
        cb = data.groupby('protein_number').agg('min')['rmsd'].to_dict()

        # Group the data by protein ID
        protein_dict = dict(tuple(data.groupby('protein_number')))
        results = []

        ranking_tracking = {
            key : {
                'rankings' : [],
                'values' : []
            } for key in ['model', 'energy']
        }

        rmsd_tracking = []

        for pid in protein_dict.keys():
            # Get decoy data for given protein and find crystal ball
            decoys = protein_dict[pid]
            protein_name = protein_dict[pid].iloc[0]['protein_id']
            cb_rmsd = cb[pid]
            decoys['cb_rmsd'] = cb_rmsd

            decoys = decoys.sort_values(by=['rmsd'])
            decoys['real_ranking'] = np.arange(1, decoys.shape[0] + 1)

            # Ranking w.r.t. model
            decoys = decoys.sort_values(by=['expected_rmsd'])
            decoys['model_ranking'] = np.arange(1, decoys.shape[0] + 1)
            decoys['d(rmsd(m,d))^2'] = np.power(decoys['expected_rmsd'] - decoys['rmsd'], 2)
            decoys['d(rmsd(m,cb))^2'] = np.power(decoys['expected_rmsd'] - decoys['cb_rmsd'], 2)
            model_rmsd = decoys[decoys['model_ranking'] == 1].iloc[0]['rmsd']
            # add to tracking
            ranking_tracking['model']['rankings'].append(decoys['real_ranking'].values)
            ranking_tracking['model']['values'].append(decoys['d(rmsd(m,cb))^2'].values)

            # Ranking w.r.t. energy function
            decoys = decoys.sort_values(by=['energy_evaluation'])
            decoys['energy_ranking'] = np.arange(1, decoys.shape[0] + 1)
            decoys['d(rmsd(e,cb))^2'] = np.power(decoys['rmsd'] - decoys['cb_rmsd'], 2)
            energy_rmsd = decoys[decoys['energy_ranking'] == 1].iloc[0]['rmsd']

            # add to tracking
            ranking_tracking['energy']['rankings'].append(decoys['real_ranking'].values)
            ranking_tracking['energy']['values'].append(decoys['d(rmsd(e,cb))^2'].values)
            rmsd_tracking.append({'protein_name' : protein_name, 'model_rmsd' : model_rmsd, 'energy_rmsd' : energy_rmsd, 'cb_rmsd' : cb_rmsd})

            results.append(decoys)

        
        # Combine the results for all proteins
        master_results = pd.concat(results)
        rmsd_results = pd.DataFrame(rmsd_tracking)

        # Boolean indexing for model's top choice and energy function's top choice
        model_selections_idx = (master_results['model_ranking'] == 1)
        energy_selections_idx = (master_results['energy_ranking'] == 1)

        # Compute the evaluation metrics
        metrics = {
            'mean(d(rmsd(m,d))^2)': master_results[model_selections_idx]['d(rmsd(m,d))^2'].mean(),
            'mean(d(rmsd(m,cb))^2)': master_results[model_selections_idx]['d(rmsd(m,cb))^2'].mean(),
            'mean(d(rmsd(e,cb))^2)': master_results[energy_selections_idx]['d(rmsd(e,cb))^2'].mean(),
            'mean_rmsd_model': master_results[model_selections_idx]['rmsd'].mean(),
            'mean_rmsd_energy': master_results[energy_selections_idx]['rmsd'].mean()
        }
        #metrics.update(clf_results)

        # define filepaths
        clf_metrics_path = self._get_next_filename(directory = self.result_directory, prefix = f'{train_test}_clf_results', suffix = 'pickle')
        metrics_path = self._get_next_filename(directory = self.result_directory, prefix = f'{train_test}_mse_results', suffix = 'json')
        master_results_path = self._get_next_filename(directory = self.result_directory, prefix = f'{train_test}_prediction_results', suffix = 'csv')
        rmsd_path = self._get_next_filename(directory = self.result_directory, prefix = f'{train_test}_rmsd_results', suffix = 'csv') 

        with open(clf_metrics_path, "wb") as file:
            # Write the dictionary to the pickle file
            pickle.dump(clf_results, file)

        # write files
        with open(metrics_path, "w") as json_file:
            json.dump(metrics, json_file)
        master_results.to_csv(master_results_path, index = False)
        rmsd_results.to_csv(rmsd_path, index = False)
        return metrics

    def _evaluate_loader(self, loader: Loader, **kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a loader and collect the evaluation results.

        Args:
            loader (Loader): The loader object to iterate over for evaluation.
            **kwargs: Additional keyword arguments for customization.

        Returns:
            Dict[str, Any]: A dictionary containing the evaluation results.
                - 'batch_idx': List of batch indices.
                - 'y_pred_cts': Concatenated array of predicted values.
                - 'y_prob': Stacked array of predicted probabilities.
                - 'y_true': Stacked array of true values.
                - 'decoy_id': Array of decoy IDs based on the number of collected results.
                - 'protein_id': Array of protein IDs based on the number of collected results.
                - 'n': Total number of collected results.
        """
        results = {
            'batch_idx' : [],
            'y_pred_cts' : [],
            'y_prob' : [],
            'y_true' : []
        }
        
        n = 0
        for idx, batch in tqdm(enumerate(loader)):
            inputs, target = batch
            y = self.model(inputs, training = False)
            y_pred = get_expected_rmsd(pdist = y)
            results['batch_idx'].append(idx)
            results['y_pred_cts'].append(y_pred)
            results['y_prob'].append(y.numpy())
            results['y_true'].append(target)
            n += y_pred.shape[0]
        
        results['y_pred_cts'] = np.concatenate(results['y_pred_cts'])
        results['y_prob'] = np.vstack(results['y_prob'])
        results['y_true'] = np.vstack(results['y_true'])
        results['decoy_id'] = np.arange(0, n) % 500
        results['protein_id'] = np.arange(0, n) // 500
        results['n'] = n
        return results

    def _evaluate_train(self, **kwargs: Dict[str, Any]) -> None:
        """
        Evaluate the model on the training data using the provided loader and evaluation parameters.

        Args:
            **kwargs (Dict[str, Any]): Additional keyword arguments for evaluation.

        Raises:
            AttributeError: If the `loader_tr` attribute is not defined.

        Returns:
            None
        """
        if not hasattr(self, 'loader_tr'):
            raise AttributeError('TrainTestModule has no attribute: loader_tr')
        
        self.loader_tr.epochs = 1
        self.train_results = self._evaluate_loader(loader = self.loader_tr, **kwargs)
        print('completed: _evaluate_loader')
        self._evaluate_results(train_test = 'train', **kwargs)
        print('completed: _evaluate_results')

    def _evaluate_test(self, **kwargs: Dict[str, Any]) -> None:
        """
        Evaluate the model on the test data using the provided loader and evaluation parameters.

        Args:
            **kwargs (Dict[str, Any]): Additional keyword arguments for evaluation.

        Raises:
            AttributeError: If the `loader_te` attribute is not defined.

        Returns:
            None
        """
        if not hasattr(self, 'loader_te'):
            raise AttributeError('TrainTestModule has no attribute: loader_te')
        
        self.loader_te.epochs = 1
        self.test_results = self._evaluate_loader(loader = self.loader_te, **kwargs)
        print('completed: _evaluate_loader')
        self._evaluate_results(train_test = 'test', **kwargs)
        print('completed: _evaluate_results')

    def train(self, epochs: int, **kwargs: Dict[str, Any]) -> None:
        """
        Train the model and evaluate on the training set.

        Args:
            **kwargs: Additional keyword arguments for customization.
        """
        self._train(epochs = epochs, **kwargs)
        print('completed: _train')
        self._reset_train_loader(epochs = 1)
        print('completed: _reset_train_loader')
        self._evaluate_train(**kwargs)
        print('completed: _evaluate_train')

    def test(self, **kwargs: Dict[str, Any]) -> None:
        """
        Evaluate the model on the test set.

        Args:
            **kwargs: Additional keyword arguments for customization.
        """
        self._reset_test_loader(epochs = 1)
        self._evaluate_test(**kwargs)

    def initial_train(self, model: Model, batch_size_tr: Optional[int] = 32, epochs_tr: Optional[int] = 10, **kwargs: Dict[str, Any]) -> None:
        self._set_training_params(params = {'train_size' : 1, 'batch_size_tr' : batch_size_tr, 'epochs_tr' : epochs_tr})
        self._set_train_loader(train_size = 1, batch_size = batch_size_tr, epochs = epochs_tr)
        self.set_model(model = model, **kwargs)
        self.train(train_size = 1, batch_size_tr = batch_size_tr, epochs_tr = epochs_tr)

    def continue_train(self, model_name: str, batch_size_tr: Optional[int] = 32, epochs_tr: Optional[int] = 10, **kwargs: Dict[str, Any]) -> None:
        self._set_training_params(params = {'train_size' : 1, 'batch_size_tr' : batch_size_tr, 'epochs_tr' : epochs_tr})
        self._set_train_loader(train_size = 1, batch_size = batch_size_tr, epochs = epochs_tr)
        self.set_model(model = model_name, load_weights = True)
        self.train(train_size = 1, batch_size_tr = batch_size_tr, epochs_tr = epochs_tr)


    
    def run(self, 
            model: tf.keras.models.Model, 
            train_size: Optional[float] = 0.8,
            epochs_tr: Optional[int] = 10,
            **kwargs: Dict[str, Any]):
        
        #self._set_gpu_config()
        t0 = time.time()
        print('_set_training_params')
        t1 = time.time()
        print('_set_train_test_loaders')
        self._set_train_test_loaders(train_size = train_size, **kwargs)
        t2 = time.time()
        print('set_model')
        self.set_model(model = model, **kwargs)
        t3 = time.time()
        print(f'_set_training_params: {t1-t0:.4}, _set_train_test_loaders: {t2-t1:.4}, set_model: {t3-t2:.4}')
        self.train(train_size = train_size, epochs = epochs_tr)
        print('completed: train')
        self.loader_tr = None
        self.train_results = None
        self.test()
        




if __name__ == '__main__':
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
        try:
            tf.config.set_logical_device_configuration(
                gpus[0],
                [tf.config.LogicalDeviceConfiguration(memory_limit = 2 * 4096)])
            logical_gpus = tf.config.list_logical_devices('GPU')
            print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
        except RuntimeError as e:
            # Virtual devices must be set before GPUs have been initialized
            print(e)